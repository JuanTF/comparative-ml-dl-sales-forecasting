{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BLOQUE process_data"
      ],
      "metadata": {
        "id": "igkZOuy7ymUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3DGulowyOLc",
        "outputId": "06bf8491-bbb5-4aea-8d2b-366c09afaadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "\n",
        "def pre_process(data_dir: str | Path):\n",
        "    \"\"\"\n",
        "    Lee y prepara los datos desde `data_dir`, que debe contener:\n",
        "      - train.csv\n",
        "      - stores.csv\n",
        "      - features.csv\n",
        "    Devuelve: data limpia, columnas categóricas, columnas numéricas, y cardinalidades.\n",
        "    \"\"\"\n",
        "    data_dir = Path(data_dir)\n",
        "\n",
        "    # Lectura de archivos desde Google Drive / carpeta local\n",
        "    data = pd.read_csv(data_dir / 'train.csv')\n",
        "    stores = pd.read_csv(data_dir / 'stores.csv')\n",
        "    features = pd.read_csv(data_dir / 'features.csv')\n",
        "\n",
        "    # Missing values en features\n",
        "    features['CPI'] = features['CPI'].fillna(features['CPI'].median())\n",
        "    features['Unemployment'] = features['Unemployment'].fillna(features['Unemployment'].median())\n",
        "\n",
        "    # MarkDowns: negativos -> 0 y NaN -> 0\n",
        "    for i in range(1, 6):\n",
        "      col = f\"MarkDown{i}\"\n",
        "      if col in features.columns:\n",
        "          features[col] = features[col].apply(lambda x: 0 if pd.notna(x) and x < 0 else x)\n",
        "          features[col] = features[col].fillna(0)\n",
        "\n",
        "    # Merge\n",
        "    data = pd.merge(data, stores, on='Store', how='left')\n",
        "    data = pd.merge(data, features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    # Tiempos y orden\n",
        "    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
        "    data.sort_values(by=['Date'], inplace=True)\n",
        "    data.set_index(data['Date'], inplace=True)\n",
        "\n",
        "    # Consolidar IsHoliday (viene duplicado del merge)\n",
        "    if 'IsHoliday_x' in data.columns:\n",
        "        data.drop(columns='IsHoliday_x', inplace=True)\n",
        "    if 'IsHoliday_y' in data.columns:\n",
        "        data.rename(columns={\"IsHoliday_y\": \"IsHoliday\"}, inplace=True)\n",
        "\n",
        "    # Derivadas de fecha\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "\n",
        "    # Agregados por Store/Dept para outliers\n",
        "    agg_data = (data.groupby(['Store', 'Dept'])\n",
        "                    .Weekly_Sales.agg(['max', 'min', 'mean', 'median', 'std'])\n",
        "                    .reset_index())\n",
        "    store_data = pd.merge(left=data, right=agg_data, on=['Store', 'Dept'], how='left')\n",
        "    store_data.dropna(inplace=True)\n",
        "\n",
        "    data = store_data.copy()\n",
        "    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
        "    data.sort_values(by=['Date'], inplace=True)\n",
        "    data.set_index(data['Date'], inplace=True)\n",
        "\n",
        "    # Total_MarkDown y limpieza de columnas\n",
        "    md_cols = [f\"MarkDown{i}\" for i in range(1, 6) if f\"MarkDown{i}\" in data.columns]\n",
        "    data['Total_MarkDown'] = data[md_cols].sum(axis=1) if md_cols else 0\n",
        "    for c in md_cols:\n",
        "        data.drop(columns=c, inplace=True)\n",
        "\n",
        "    # Filtro de outliers sobre numéricas clave\n",
        "    numeric_col = ['Weekly_Sales', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Total_MarkDown']\n",
        "    data_numeric = data[numeric_col].copy()\n",
        "    data = data[(np.abs(stats.zscore(data_numeric, nan_policy='omit')) < 2.5).all(axis=1)]\n",
        "    data = data[data['Weekly_Sales'] >= 0]\n",
        "\n",
        "    # Tipos\n",
        "    if 'IsHoliday' in data.columns:\n",
        "        data['IsHoliday'] = data['IsHoliday'].astype('int')\n",
        "\n",
        "    # Categóricas: factorize\n",
        "    cat_col = ['Store', 'Dept', 'Type']\n",
        "    num_of_unique = []\n",
        "    for col in cat_col:\n",
        "        num_of_unique.append(data[col].nunique())\n",
        "        data[col] = pd.factorize(data[col])[0]\n",
        "\n",
        "    # Numéricas finales (las que usará el modelo)\n",
        "    num_col = ['Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "               'Total_MarkDown', 'max', 'min', 'mean', 'median', 'std']\n",
        "\n",
        "    return data, cat_col, num_col, num_of_unique\n",
        "\n",
        "def get_data(data_dir: str | Path,\n",
        "             test_size: float = 0.20,\n",
        "             valid_size: float = 0.15,\n",
        "             seed: int = 0):\n",
        "    \"\"\"\n",
        "    Devuelve splits aleatorios (train/valid/test) listos para DataLoader.\n",
        "    Nota: este split es ALEATORIO (sirve para probar el pipeline). Para replicar el paper 1:1,\n",
        "    luego cambiaremos a split temporal 113/15/15 + ventana creciente.\n",
        "    \"\"\"\n",
        "    data, cat_col, num_col, num_of_unique = pre_process(data_dir)\n",
        "\n",
        "    X = data.drop(['Weekly_Sales'], axis=1)\n",
        "    y = data['Weekly_Sales'].astype(float)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, shuffle=True\n",
        "    )\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X_train, y_train, test_size=valid_size, random_state=seed, shuffle=True\n",
        "    )\n",
        "\n",
        "    print(\"Training set size:\", len(X_train))\n",
        "    print(\"Validation set size:\", len(X_valid))\n",
        "    print(\"Test set size:\", len(X_test))\n",
        "\n",
        "    return X_train, X_valid, X_test, y_train, y_valid, y_test, num_of_unique, cat_col, num_col\n"
      ],
      "metadata": {
        "id": "D9z9g5hZyOsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOQUE 0 — Preparación"
      ],
      "metadata": {
        "id": "yPAjFXNduS3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# BLOQUE 0 — Preparación (paper 113/15/15 + preprocesamiento sin fuga)\n",
        "# =========================\n",
        "import time, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# --- Directorios (ajusta si fuera necesario)\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/Modelo/outputs\")\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/Modelo/data\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Usando SAVE_DIR =\", SAVE_DIR)\n",
        "\n",
        "# --- 0.1) Cargar y preparar datos con tu pre_process del notebook\n",
        "# Debe devolver: data, cat_col, num_col, num_of_unique\n",
        "data, cat_col, num_col, num_of_unique = pre_process(DATA_DIR)\n",
        "\n",
        "# =======================\n",
        "# Normalizar 'Date' (arreglo ambigüedad índice/columna)\n",
        "# =======================\n",
        "# Si el índice se llama 'Date', quitar ese nombre para evitar colisiones\n",
        "if getattr(data.index, \"name\", None) == \"Date\":\n",
        "    data.index.name = None\n",
        "\n",
        "# Si 'Date' es parte del índice (MultiIndex), soltar ese nivel\n",
        "if hasattr(data.index, \"names\") and \"Date\" in (data.index.names or []):\n",
        "    try:\n",
        "        data = data.reset_index(level=\"Date\", drop=True)\n",
        "    except Exception:\n",
        "        data = data.reset_index(drop=True)\n",
        "\n",
        "# Asegurar que 'Date' es columna y ordenar temporalmente\n",
        "if 'Date' not in data.columns:\n",
        "    # si venía como índice (simple) y no existe la columna, lo reconstruimos\n",
        "    # (en tu pre_process normalmente ya existe)\n",
        "    raise ValueError(\"No se encontró columna 'Date' después de normalizar. Revisa pre_process().\")\n",
        "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
        "data = data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# =======================\n",
        "# Columnas efectivas (como en el paper)\n",
        "# =======================\n",
        "cat_features = [c for c in ['Store', 'Dept', 'Type'] if c in data.columns]\n",
        "extra_num    = [c for c in ['IsHoliday', 'Year', 'Month'] if c in data.columns]\n",
        "num_features = [c for c in num_col if c in data.columns] + extra_num\n",
        "\n",
        "# =======================\n",
        "# Split temporal 113/15/15\n",
        "# =======================\n",
        "uniq_weeks = np.array(sorted(data['Date'].unique()))\n",
        "assert len(uniq_weeks) >= 113 + 15 + 15, \"No hay suficientes semanas para 113/15/15.\"\n",
        "\n",
        "weeks_train = uniq_weeks[:113]\n",
        "weeks_valid = uniq_weeks[113:113+15]\n",
        "weeks_test  = uniq_weeks[113+15:113+15+15]\n",
        "\n",
        "mask_weeks = lambda df, weeks: df['Date'].isin(weeks)\n",
        "\n",
        "df_train = data[mask_weeks(data, weeks_train)].copy()\n",
        "df_valid = data[mask_weeks(data, weeks_valid)].copy()\n",
        "df_test  = data[mask_weeks(data, weeks_test)].copy()\n",
        "\n",
        "# =======================\n",
        "# Preprocesador SIN FUGA (fit en train, transform en valid/test)\n",
        "# =======================\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(with_mean=False), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "drop_cols = ['Weekly_Sales', 'Date']\n",
        "\n",
        "X_train = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])\n",
        "y_train = df_train['Weekly_Sales'].astype(float)\n",
        "\n",
        "X_valid = df_valid.drop(columns=[c for c in drop_cols if c in df_valid.columns])\n",
        "y_valid = df_valid['Weekly_Sales'].astype(float)\n",
        "\n",
        "X_test  = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n",
        "y_test  = df_test['Weekly_Sales'].astype(float)\n",
        "\n",
        "Xtr = preprocess.fit_transform(X_train, y_train)\n",
        "Xva = preprocess.transform(X_valid)\n",
        "Xte = preprocess.transform(X_test)\n",
        "\n",
        "print(\"Shapes → Xtr:\", Xtr.shape, \"| Xva:\", Xva.shape, \"| Xte:\", Xte.shape)\n",
        "\n",
        "# =======================\n",
        "# Métricas y evaluador común (para usar desde cada bloque de modelo)\n",
        "# =======================\n",
        "def mape(y_true, y_pred, eps=1e-8):\n",
        "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "def eval_on_mats(name, model):\n",
        "    t0 = time.time()\n",
        "    model.fit(Xtr, y_train)\n",
        "    dt = time.time() - t0\n",
        "    def rep(X, y):\n",
        "        p = model.predict(X)\n",
        "        mse = mean_squared_error(y, p); rmse = np.sqrt(mse)\n",
        "        return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mean_absolute_error(y,p),\n",
        "                \"MAPE\": mape(y,p), \"R2\": r2_score(y,p)}\n",
        "    print(f\"[{name}] entrenado en {dt/60:.1f} min\")\n",
        "    return rep(Xva, y_valid), rep(Xte, y_test)\n",
        "\n",
        "# =======================\n",
        "# Utilidades de guardado/recarga para no perder trabajo\n",
        "# =======================\n",
        "!pip -q install joblib scipy\n",
        "import joblib\n",
        "from scipy import sparse\n",
        "\n",
        "def save_artifacts():\n",
        "    joblib.dump(preprocess, SAVE_DIR / \"preprocess.joblib\")\n",
        "    sparse.save_npz(SAVE_DIR / \"Xtr.npz\", Xtr)\n",
        "    sparse.save_npz(SAVE_DIR / \"Xva.npz\", Xva)\n",
        "    sparse.save_npz(SAVE_DIR / \"Xte.npz\", Xte)\n",
        "    np.save(SAVE_DIR / \"y_train.npy\", y_train.values if hasattr(y_train, \"values\") else y_train)\n",
        "    np.save(SAVE_DIR / \"y_valid.npy\", y_valid.values if hasattr(y_valid, \"values\") else y_valid)\n",
        "    np.save(SAVE_DIR / \"y_test.npy\",  y_test.values  if hasattr(y_test,  \"values\") else y_test)\n",
        "    meta = {\n",
        "        \"cat_features\": cat_features,\n",
        "        \"num_features\": num_features,\n",
        "        \"weeks_train\": weeks_train.tolist(),\n",
        "        \"weeks_valid\": weeks_valid.tolist(),\n",
        "        \"weeks_test\":  weeks_test.tolist()\n",
        "    }\n",
        "    with open(SAVE_DIR / \"metadata.json\", \"w\") as f:\n",
        "        json.dump(meta, f, indent=2, default=str)\n",
        "    print(\"✓ Preprocesador, matrices y metadatos guardados en\", SAVE_DIR)\n",
        "\n",
        "def load_artifacts():\n",
        "    global preprocess, Xtr, Xva, Xte, y_train, y_valid, y_test\n",
        "    preprocess = joblib.load(SAVE_DIR / \"preprocess.joblib\")\n",
        "    Xtr = sparse.load_npz(SAVE_DIR / \"Xtr.npz\")\n",
        "    Xva = sparse.load_npz(SAVE_DIR / \"Xva.npz\")\n",
        "    Xte = sparse.load_npz(SAVE_DIR / \"Xte.npz\")\n",
        "    y_train = np.load(SAVE_DIR / \"y_train.npy\")\n",
        "    y_valid = np.load(SAVE_DIR / \"y_valid.npy\")\n",
        "    y_test  = np.load(SAVE_DIR / \"y_test.npy\")\n",
        "    print(\"✓ Artifacts recargados desde\", SAVE_DIR)\n",
        "\n",
        "# =======================\n",
        "# Contenedores de resultados en memoria (opcional)\n",
        "# =======================\n",
        "import pandas as pd, json\n",
        "\n",
        "def save_metrics(nombre_modelo, val_dict, test_dict):\n",
        "    row = {\"Modelo\": nombre_modelo,\n",
        "           **{f\"val_{k}\": v for k, v in val_dict.items()},\n",
        "           **{f\"test_{k}\": v for k, v in test_dict.items()}}\n",
        "    csv_path = SAVE_DIR / \"resultados_modelos.csv\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    except FileNotFoundError:\n",
        "        df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    with open(SAVE_DIR / f\"metrics_{nombre_modelo}.json\", \"w\") as f:\n",
        "        json.dump({\"val\": val_dict, \"test\": test_dict}, f, indent=2)\n",
        "    print(f\"✓ Métricas guardadas en {csv_path} y en metrics_{nombre_modelo}.json\")\n",
        "\n",
        "\n",
        "RESULTS_VAL, RESULTS_TEST = {}, {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gojSGBYpwqBK",
        "outputId": "97804582-94c8-4052-c686-da999139e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando SAVE_DIR = /content/drive/MyDrive/Modelo/outputs\n",
            "Shapes → Xtr: (290842, 143) | Xva: (42285, 143) | Xte: (41120, 143)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "\n",
        "def summarize_runs(rows):\n",
        "    # rows = lista de dicts con claves MSE, RMSE, MAE, MAPE, R2\n",
        "    keys = [\"MSE\",\"RMSE\",\"MAE\",\"MAPE\",\"R2\"]\n",
        "    mean = {k: np.mean([r[k] for r in rows]) for k in keys}\n",
        "    std  = {k: np.std( [r[k] for r in rows], ddof=1) for k in keys}\n",
        "    return mean, std\n",
        "\n",
        "def fmt_pm(mean: dict, std: dict):\n",
        "    out = {}\n",
        "    for k in mean:                 # k = 'MSE', 'RMSE', ...\n",
        "        m = float(mean[k])\n",
        "        s = float(std[k])\n",
        "        if k == \"R2\":\n",
        "            out[k] = f\"{m:.3f} ± {s:.3f}\"\n",
        "        else:\n",
        "            out[k] = f\"{m:.2f} ± {s:.2f}\"\n",
        "    return out"
      ],
      "metadata": {
        "id": "aupF92pf_UZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 1 — Linear Regression"
      ],
      "metadata": {
        "id": "aMvKsfeSumu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "runs_val, runs_test = [], []\n",
        "for seed in range(15):\n",
        "    val, test = eval_on_mats(\"LinearRegression\", LinearRegression())\n",
        "    runs_val.append(val); runs_test.append(test)\n",
        "\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"LinearRegression — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"LinearRegression — Test :\", fmt_pm(mean_test, std_test))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame([fmt_pm(mean_test, std_test)], index=[\"LinearRegression (Test)\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "BH3gdBhPudpt",
        "outputId": "48ab27af-1209-49a5-9f12-580f13154bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.0 min\n",
            "[LinearRegression] entrenado en 0.1 min\n",
            "LinearRegression — Valid: {'MSE': '19066684.43 ± 0.00', 'RMSE': '4366.54 ± 0.00', 'MAE': '2291.31 ± 0.00', 'MAPE': '2259816842.85 ± 0.00', 'R2': '0.922 ± 0.000'}\n",
            "LinearRegression — Test : {'MSE': '14202434.57 ± 0.00', 'RMSE': '3768.61 ± 0.00', 'MAE': '2167.10 ± 0.00', 'MAPE': '693817530.55 ± 0.00', 'R2': '0.939 ± 0.000'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        MSE            RMSE             MAE  \\\n",
              "LinearRegression (Test)  14202434.57 ± 0.00  3768.61 ± 0.00  2167.10 ± 0.00   \n",
              "\n",
              "                                        MAPE             R2  \n",
              "LinearRegression (Test)  693817530.55 ± 0.00  0.939 ± 0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c62fe661-dc60-4207-9da4-1649b2a35cdf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LinearRegression (Test)</th>\n",
              "      <td>14202434.57 ± 0.00</td>\n",
              "      <td>3768.61 ± 0.00</td>\n",
              "      <td>2167.10 ± 0.00</td>\n",
              "      <td>693817530.55 ± 0.00</td>\n",
              "      <td>0.939 ± 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c62fe661-dc60-4207-9da4-1649b2a35cdf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c62fe661-dc60-4207-9da4-1649b2a35cdf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c62fe661-dc60-4207-9da4-1649b2a35cdf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"MSE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"14202434.57 \\u00b1 0.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"3768.61 \\u00b1 0.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2167.10 \\u00b1 0.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"693817530.55 \\u00b1 0.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0.939 \\u00b1 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar tabla estilo paper (±) a CSV acumulativo\n",
        "import pandas as pd\n",
        "row_valid = fmt_pm(mean_val, std_val)\n",
        "row_test  = fmt_pm(mean_test, std_test)\n",
        "df_pm = pd.DataFrame([row_valid, row_test],\n",
        "                     index=[\"LinearRegression (Valid)\", \"LinearRegression (Test)\"])\n",
        "\n",
        "csv_pm = SAVE_DIR / \"resultados_resumen_pm.csv\"\n",
        "try:\n",
        "    old = pd.read_csv(csv_pm, index_col=0)\n",
        "    out = pd.concat([old, df_pm]).drop_duplicates()\n",
        "except FileNotFoundError:\n",
        "    out = df_pm\n",
        "out.to_csv(csv_pm)\n",
        "print(\"✓ Resumen ± guardado en\", csv_pm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhuZhyB1EGcr",
        "outputId": "5921319f-ca7d-4cec-d6fa-cd831279d26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Resumen ± guardado en /content/drive/MyDrive/Modelo/outputs/resultados_resumen_pm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_artifacts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acp5z-sNEP1i",
        "outputId": "e8704383-1dcd-4628-dad9-1e918ab730c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Preprocesador, matrices y metadatos guardados en /content/drive/MyDrive/Modelo/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 2 - XGBoost"
      ],
      "metadata": {
        "id": "_BDssjUU16-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "\n",
        "runs_val, runs_test = [], []\n",
        "for _ in range(15):\n",
        "    v, t = eval_on_mats(\"XGBoost_baseline\", XGBRegressor())\n",
        "    runs_val.append(v); runs_test.append(t)\n",
        "\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"XGBoost — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"XGBoost — Test :\", fmt_pm(mean_test, std_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_14lDxM19oR",
        "outputId": "6819e685-2874-4f63-862f-f256fc74ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "[XGBoost_baseline] entrenado en 0.1 min\n",
            "XGBoost — Valid: {'MSE': '9744079.75 ± 0.00', 'RMSE': '3121.55 ± 0.00', 'MAE': '1747.51 ± 0.00', 'MAPE': '715932120.91 ± 0.00', 'R2': '0.960 ± 0.000'}\n",
            "XGBoost — Test : {'MSE': '9131063.16 ± 0.00', 'RMSE': '3021.76 ± 0.00', 'MAE': '1766.86 ± 0.00', 'MAPE': '531546132.87 ± 0.00', 'R2': '0.961 ± 0.000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar tabla estilo paper (±)\n",
        "row_valid = fmt_pm(mean_val, std_val)\n",
        "row_test  = fmt_pm(mean_test, std_test)\n",
        "df_pm = pd.DataFrame([row_valid, row_test],\n",
        "                     index=[\"XGBoost (Valid)\", \"XGBoost (Test)\"])\n",
        "\n",
        "csv_pm = SAVE_DIR / \"resultados_resumen_pm.csv\"\n",
        "try:\n",
        "    old = pd.read_csv(csv_pm, index_col=0)\n",
        "    out = pd.concat([old, df_pm]).drop_duplicates()\n",
        "except FileNotFoundError:\n",
        "    out = df_pm\n",
        "out.to_csv(csv_pm)\n",
        "print(\"✓ Resumen ± guardado en\", csv_pm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u30rEbpUE7YC",
        "outputId": "dfb5dd7f-aee5-4d7f-eb49-21e5b25fe075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Resumen ± guardado en /content/drive/MyDrive/Modelo/outputs/resultados_resumen_pm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOQUE DE PPEPARACIÓN DE LOS MODELOS RF y KNN"
      ],
      "metadata": {
        "id": "SG1c5vSL3mk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini-prep para modelos que no aceptan sparse (RF y kNN)\n",
        "from scipy import sparse\n",
        "import time, numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "Xtr_d = Xtr.toarray() if sparse.issparse(Xtr) else Xtr\n",
        "Xva_d = Xva.toarray() if sparse.issparse(Xva) else Xva\n",
        "Xte_d = Xte.toarray() if sparse.issparse(Xte) else Xte\n",
        "\n",
        "def eval_on_mats_dense(name, model):\n",
        "    t0 = time.time()\n",
        "    model.fit(Xtr_d, y_train)\n",
        "    dt = time.time() - t0\n",
        "    def rep(X, y):\n",
        "        p = model.predict(X)\n",
        "        mse = mean_squared_error(y, p); rmse = np.sqrt(mse)\n",
        "        return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mean_absolute_error(y,p),\n",
        "                \"MAPE\": mape(y,p), \"R2\": r2_score(y,p)}\n",
        "    print(f\"[{name}] entrenado en {dt/60:.1f} min\")\n",
        "    return rep(Xva_d, y_valid), rep(Xte_d, y_test)"
      ],
      "metadata": {
        "id": "7DCj2Wh83gon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 3 - KNN"
      ],
      "metadata": {
        "id": "CirYpxcZ10SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLOQUE DE PRIMERA EJECUCIÓN"
      ],
      "metadata": {
        "id": "OpTKUpDlEvwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import pandas as pd\n",
        "\n",
        "runs_val, runs_test = [], []\n",
        "for _ in range(15):\n",
        "    v, t = eval_on_mats(\"kNN_baseline\", KNeighborsRegressor())\n",
        "    runs_val.append(v); runs_test.append(t)\n",
        "\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"kNN — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"kNN — Test :\", fmt_pm(mean_test, std_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "G9dfPPib14i_",
        "outputId": "a4c13e40-3290-4d9e-abc0-b5363971a356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2085872907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mruns_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_on_mats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kNN_baseline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mruns_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mruns_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1419915193.py\u001b[0m in \u001b[0;36meval_on_mats\u001b[0;34m(name, model)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \"MAPE\": mape(y,p), \"R2\": r2_score(y,p)}\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{name}] entrenado en {dt/60:.1f} min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# =======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1419915193.py\u001b[0m in \u001b[0;36mrep\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mean_absolute_error(y,p),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             chunked_results = list(\n\u001b[0m\u001b[1;32m    907\u001b[0m                 pairwise_distances_chunked(\n\u001b[1;32m    908\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[1;32m   2254\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2478\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_euclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    906\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    907\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m_matmul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;34mf\"{err_prefix} (n,k={N}),(k={other.shape[0]},m)->(n,m)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 )\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m_matmul_sparse\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mo_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matmat_maxnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_indptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_indptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnnz\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_shape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar tabla estilo paper (±) en CSV acumulativo\n",
        "row_valid = fmt_pm(mean_val, std_val)\n",
        "row_test  = fmt_pm(mean_test, std_test)\n",
        "df_pm = pd.DataFrame([row_valid, row_test],\n",
        "                     index=[\"kNN (Valid)\", \"kNN (Test)\"])\n",
        "\n",
        "csv_pm = SAVE_DIR / \"resultados_resumen_pm.csv\"\n",
        "try:\n",
        "    old = pd.read_csv(csv_pm, index_col=0)\n",
        "    out = pd.concat([old, df_pm]).drop_duplicates()\n",
        "except FileNotFoundError:\n",
        "    out = df_pm\n",
        "out.to_csv(csv_pm)\n",
        "print(\"✓ Resumen ± guardado en\", csv_pm)"
      ],
      "metadata": {
        "id": "YPf-w77qGBjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLOQUE DE EJECUCIÓN OTRO DIA"
      ],
      "metadata": {
        "id": "xKNOHunRB7nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_artifacts()  # recupera preprocess, Xtr/Xva/Xte, y_*\n",
        "import joblib\n",
        "prog = joblib.load(SAVE_DIR / \"knn_progress.joblib\")\n",
        "runs_val  = prog.get(\"runs_val\", [])\n",
        "runs_test = prog.get(\"runs_test\", [])\n",
        "start_i   = len(runs_val)  # continúa desde aquí"
      ],
      "metadata": {
        "id": "HJQWCDycB6m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_artifacts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXOf9tVCH1Kt",
        "outputId": "fcbbe3c0-0161-4728-f845-cc7123de207a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Artifacts recargados desde /content/drive/MyDrive/Modelo/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, os\n",
        "prog_path = SAVE_DIR / \"knn_progress.joblib\"\n",
        "if os.path.exists(prog_path):\n",
        "    prog = joblib.load(prog_path)\n",
        "    runs_val  = prog.get(\"runs_val\", [])\n",
        "    runs_test = prog.get(\"runs_test\", [])\n",
        "    print(f\"Progreso cargado: {len(runs_val)} corridas\")\n",
        "else:\n",
        "    runs_val, runs_test = [], []\n",
        "    print(\"No había progreso previo. Empezando desde 0.\")\n",
        "\n",
        "start_i = len(runs_val)  # continúa desde aquí"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I1FQrsDEouH",
        "outputId": "56736649-a418-44fa-dab2-f2db37c5300c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progreso cargado: 5 corridas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import joblib\n",
        "\n",
        "remaining = max(0, 15 - start_i)\n",
        "print(f\"Ya tienes {start_i}/15 corridas. Ejecutando {remaining} más...\")\n",
        "\n",
        "for i in range(start_i, 15):\n",
        "    v, t = eval_on_mats(\"kNN_baseline\", KNeighborsRegressor())\n",
        "    runs_val.append(v); runs_test.append(t)\n",
        "\n",
        "    # checkpoint opcional cada 5 corridas o al final\n",
        "    if ((i + 1) % 5 == 0) or (i == 14):\n",
        "        joblib.dump({\"runs_val\": runs_val, \"runs_test\": runs_test},\n",
        "                    SAVE_DIR / \"knn_progress.joblib\")\n",
        "        print(f\"✓ Progreso guardado tras {i+1} corridas\")\n",
        "\n",
        "# Resumen ± (usa tus helpers summarize_runs y fmt_pm ya definidos)\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"kNN — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"kNN — Test :\", fmt_pm(mean_test, std_test))\n",
        "\n",
        "# (opcional) guardar también en CSV/JSON con tu helper\n",
        "save_metrics(\"kNN_15runs_mean±std\", mean_val, mean_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyURXiEqI9te",
        "outputId": "21eb2a3b-af54-41ca-f24c-ead660cea990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ya tienes 5/15 corridas. Ejecutando 10 más...\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "✓ Progreso guardado tras 10 corridas\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "[kNN_baseline] entrenado en 0.0 min\n",
            "✓ Progreso guardado tras 15 corridas\n",
            "kNN — Valid: {'MSE': '16809389.82 ± 0.00', 'RMSE': '4099.93 ± 0.00', 'MAE': '1999.61 ± 0.00', 'MAPE': '630020630.93 ± 0.00', 'R2': '0.931 ± 0.000'}\n",
            "kNN — Test : {'MSE': '12931535.93 ± 0.00', 'RMSE': '3596.04 ± 0.00', 'MAE': '1854.84 ± 0.00', 'MAPE': '292653810.88 ± 0.00', 'R2': '0.945 ± 0.000'}\n",
            "✓ Métricas guardadas en /content/drive/MyDrive/Modelo/outputs/resultados_modelos.csv y en metrics_kNN_15runs_mean±std.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GUARDADO DEL PROGRESO"
      ],
      "metadata": {
        "id": "9iPFWQJQBvoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_artifacts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiNF48Y2p5HN",
        "outputId": "fd1cbb5b-501c-4e9e-f917-118f095289a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Preprocesador, matrices y metadatos guardados en /content/drive/MyDrive/Modelo/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(\n",
        "    {\"runs_val\": runs_val, \"runs_test\": runs_test},\n",
        "    SAVE_DIR / \"knn_progress.joblib\"\n",
        ")\n",
        "print(\"✓ Progreso kNN guardado:\", SAVE_DIR / \"knn_progress.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsgZ0P73swWU",
        "outputId": "000cf5ed-7140-4639-fa70-0ea3bcf08d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Progreso kNN guardado: /content/drive/MyDrive/Modelo/outputs/knn_progress.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 4 - Random Forest"
      ],
      "metadata": {
        "id": "xJhqJDgy14zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def _rf_fast_run(seed: int):\n",
        "    rf = RandomForestRegressor(\n",
        "        # ⚡️ Hiperparámetros pro-velocidad:\n",
        "        n_estimators=100,        # 100 árboles (menos que 200/300)\n",
        "        max_depth=15,            # tope de profundidad (evita árboles muy grandes)\n",
        "        min_samples_leaf=5,      # hojas más “gruesas” = menos nodos\n",
        "        max_features=\"sqrt\",     # menos variables por split\n",
        "        bootstrap=True,\n",
        "        max_samples=0.7,         # 70% de las filas por árbol (subsampling)\n",
        "        n_jobs=1,                # importante: 1 aquí, paralelizamos afuera\n",
        "        random_state=seed\n",
        "    )\n",
        "    return eval_on_mats(\"RandomForest\", rf)\n",
        "\n",
        "# 15 corridas en paralelo (1 proceso por corrida)\n",
        "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
        "    delayed(_rf_fast_run)(s) for s in range(15)\n",
        ")\n",
        "runs_val  = [v for v, t in results]\n",
        "runs_test = [t for v, t in results]\n",
        "\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"RF_fast — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"RF_fast — Test :\", fmt_pm(mean_test, std_test))\n",
        "save_metrics(\"RandomForest_pm\", mean_val, mean_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewAYVblFMiSa",
        "outputId": "d1ffde78-9b0d-4e24-ea07-ef701d64afa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  9.7min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 23.6min\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 36.0min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF_fast — Valid: {'MSE': '17583209.31 ± 158282.68', 'RMSE': '4193.19 ± 18.90', 'MAE': '2212.91 ± 22.70', 'MAPE': '2774615691.69 ± 221478438.53', 'R2': '0.928 ± 0.001'}\n",
            "RF_fast — Test : {'MSE': '12793924.50 ± 97889.60', 'RMSE': '3576.84 ± 13.69', 'MAE': '1995.35 ± 19.27', 'MAPE': '1073470015.90 ± 137684590.52', 'R2': '0.945 ± 0.000'}\n",
            "✓ Métricas guardadas en /content/drive/MyDrive/Modelo/outputs/resultados_modelos.csv y en metrics_RandomForest_pm.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 5 - GRU-basedmodel"
      ],
      "metadata": {
        "id": "nGGYsgBAxERt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# BLOQUE 5 — GRU (15 corridas, paralelo CPU, rápido)\n",
        "# =========================\n",
        "\n",
        "# 0) Modo CPU y evitar problemas de Dynamo en Colab\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"       # fuerza CPU\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"       # evita errores de torch._dynamo\n",
        "\n",
        "# 1) Imports\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# 2) Fallbacks por si no existen en el entorno\n",
        "if 'mape' not in globals():\n",
        "    def mape(y_true, y_pred, eps=1e-8):\n",
        "        y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "        return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "def _ensure_save_metrics():\n",
        "    # define un no-op si no existe save_metrics en tu BLOQUE 0\n",
        "    if 'save_metrics' not in globals():\n",
        "        def save_metrics(name, val_dict, test_dict):\n",
        "            print(f\"(Nota) save_metrics no definido; omitiendo guardado para {name}.\")\n",
        "        globals()['save_metrics'] = save_metrics\n",
        "_ensure_save_metrics()\n",
        "\n",
        "# 3) Densificar matrices si vienen en sparse (GRU trabaja en float denso)\n",
        "Xtr_d = Xtr.toarray() if sparse.issparse(Xtr) else Xtr\n",
        "Xva_d = Xva.toarray() if sparse.issparse(Xva) else Xva\n",
        "Xte_d = Xte.toarray() if sparse.issparse(Xte) else Xte\n",
        "\n",
        "# 4) DataLoaders (cada fila como secuencia de longitud 1: (B, 1, F))\n",
        "def make_loader(X, y, bs=1024, shuffle=False):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    X = torch.from_numpy(X).unsqueeze(1)  # (N, 1, F)\n",
        "    y = torch.from_numpy(y)               # (N,)\n",
        "    ds = TensorDataset(X, y)\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=0, pin_memory=False)\n",
        "\n",
        "train_loader = make_loader(Xtr_d, y_train, bs=1024, shuffle=True)\n",
        "valid_loader = make_loader(Xva_d, y_valid, bs=2048, shuffle=False)\n",
        "test_loader  = make_loader(Xte_d, y_test,  bs=2048, shuffle=False)\n",
        "\n",
        "# 5) Modelo GRU compacto (rápido en CPU)\n",
        "class GRUTabular(nn.Module):\n",
        "    def __init__(self, input_size, hidden=64, layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden,\n",
        "            num_layers=layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.0 if layers == 1 else dropout\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):            # x: (B, 1, F)\n",
        "        out, _ = self.gru(x)         # (B, 1, H)\n",
        "        h = out[:, -1, :]            # (B, H)\n",
        "        return self.head(h).squeeze(1)  # (B,)\n",
        "\n",
        "# 6) Loop de entrenamiento y evaluación\n",
        "def train_epoch(model, loader, opt, lossf, device):\n",
        "    model.train()\n",
        "    tot = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = lossf(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        tot += loss.item() * xb.size(0)\n",
        "    return tot / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        ps.append(pred); ys.append(yb.numpy())\n",
        "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
        "    mse  = mean_squared_error(y, p)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae  = mean_absolute_error(y, p)\n",
        "    mp   = mape(y, p)\n",
        "    r2   = r2_score(y, p)\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mp, \"R2\": r2}\n",
        "\n",
        "# 7) Helpers para media ± desviación\n",
        "def summarize_runs(dicts_list):\n",
        "    keys = dicts_list[0].keys()\n",
        "    mean = {k: float(np.mean([d[k] for d in dicts_list])) for k in keys}\n",
        "    std  = {k: float(np.std([d[k] for d in dicts_list], ddof=1)) for k in keys}\n",
        "    return mean, std\n",
        "\n",
        "def fmt_pm(mean, std, r2_dec=3):\n",
        "    out = {}\n",
        "    for k in mean.keys():\n",
        "        if k == \"R2\":\n",
        "            out[k] = f\"{mean[k]:.{r2_dec}f} ± {std[k]:.{r2_dec}f}\"\n",
        "        else:\n",
        "            out[k] = f\"{mean[k]:.2f} ± {std[k]:.2f}\"\n",
        "    return out\n",
        "\n",
        "# 8) Función de una corrida (CPU) — parámetros “rápidos”\n",
        "def run_one_gru_cpu(seed: int, epochs=3, hidden=64):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    model = GRUTabular(input_size=Xtr_d.shape[1], hidden=hidden, layers=1).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss  = torch.nn.MSELoss()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        _ = train_epoch(model, train_loader, opt, loss, device)\n",
        "\n",
        "    val = eval_metrics(model, valid_loader, device)\n",
        "    tst = eval_metrics(model, test_loader,  device)\n",
        "    return val, tst\n",
        "\n",
        "# 9) Ejecutar 15 corridas en paralelo (CPU)\n",
        "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
        "    delayed(run_one_gru_cpu)(s, epochs=3, hidden=64) for s in range(15)\n",
        ")\n",
        "runs_val  = [v for v, t in results]\n",
        "runs_test = [t for v, t in results]\n",
        "\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"GRU_fast — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"GRU_fast — Test :\", fmt_pm(mean_test, std_test))\n",
        "\n",
        "# 10) Guardar métricas (usando tu save_metrics del BLOQUE 0 si existe)\n",
        "save_metrics(\"GRU_fast_15runs_pm\", mean_val, mean_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOGagIHhxAok",
        "outputId": "eff257cc-afc2-462f-bddf-55b97515f7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.0min\n",
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU_fast — Valid: {'MSE': '395330048.00 ± 4051131.10', 'RMSE': '19882.67 ± 101.73', 'MAE': '12537.66 ± 104.65', 'MAPE': '1761501568.00 ± 272532832.00', 'R2': '-0.614 ± 0.017'}\n",
            "GRU_fast — Test : {'MSE': '372576597.33 ± 3881662.34', 'RMSE': '19302.00 ± 100.41', 'MAE': '12032.03 ± 102.54', 'MAPE': '1811410304.00 ± 280254496.00', 'R2': '-0.588 ± 0.017'}\n",
            "✓ Métricas guardadas en /content/drive/MyDrive/Modelo/outputs/resultados_modelos.csv y en metrics_GRU_fast_15runs_pm.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 6 LSTM-basedmodel"
      ],
      "metadata": {
        "id": "gLDFnx4CbYsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# BLOQUE 6 — LSTM (15 corridas, paralelo CPU, rápido)\n",
        "# =========================\n",
        "\n",
        "# 0) Forzar CPU y evitar problemas de Dynamo en Colab\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # fuerza CPU\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"   # evita errores de torch._dynamo\n",
        "\n",
        "# 1) Imports\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# 2) Fallbacks si faltan helpers del BLOQUE 0\n",
        "if 'mape' not in globals():\n",
        "    def mape(y_true, y_pred, eps=1e-8):\n",
        "        y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "        return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "if 'summarize_runs' not in globals():\n",
        "    def summarize_runs(dicts_list):\n",
        "        keys = dicts_list[0].keys()\n",
        "        mean = {k: float(np.mean([d[k] for d in dicts_list])) for k in keys}\n",
        "        std  = {k: float(np.std([d[k] for d in dicts_list], ddof=1)) for k in keys}\n",
        "        return mean, std\n",
        "\n",
        "if 'fmt_pm' not in globals():\n",
        "    def fmt_pm(mean, std, r2_dec=3):\n",
        "        out = {}\n",
        "        for k in mean.keys():\n",
        "            if k == \"R2\":\n",
        "                out[k] = f\"{mean[k]:.{r2_dec}f} ± {std[k]:.{r2_dec}f}\"\n",
        "            else:\n",
        "                out[k] = f\"{mean[k]:.2f} ± {std[k]:.2f}\"\n",
        "        return out\n",
        "\n",
        "if 'save_metrics' not in globals():\n",
        "    def save_metrics(name, val_dict, test_dict):\n",
        "        print(f\"(Nota) save_metrics no definido; omitiendo guardado para {name}.\")\n",
        "\n",
        "# 3) Densificar matrices si vienen en sparse (LSTM trabaja en float denso)\n",
        "Xtr_d = Xtr.toarray() if sparse.issparse(Xtr) else Xtr\n",
        "Xva_d = Xva.toarray() if sparse.issparse(Xva) else Xva\n",
        "Xte_d = Xte.toarray() if sparse.issparse(Xte) else Xte\n",
        "\n",
        "# 4) DataLoaders (cada fila como secuencia de longitud 1: (B, 1, F))\n",
        "def make_loader(X, y, bs=1024, shuffle=False):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    X = torch.from_numpy(X).unsqueeze(1)  # (N, 1, F)\n",
        "    y = torch.from_numpy(y)               # (N,)\n",
        "    ds = TensorDataset(X, y)\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=0, pin_memory=False)\n",
        "\n",
        "# reutiliza loaders si ya existen; si no, créalos\n",
        "if not all(k in globals() for k in ['train_loader','valid_loader','test_loader']):\n",
        "    train_loader = make_loader(Xtr_d, y_train, bs=1024, shuffle=True)\n",
        "    valid_loader = make_loader(Xva_d, y_valid, bs=2048, shuffle=False)\n",
        "    test_loader  = make_loader(Xte_d, y_test,  bs=2048, shuffle=False)\n",
        "\n",
        "# 5) Modelo LSTM compacto (rápido en CPU)\n",
        "class LSTMTabular(nn.Module):\n",
        "    def __init__(self, input_size, hidden=64, layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden,\n",
        "            num_layers=layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.0 if layers == 1 else dropout\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):           # x: (B, 1, F)\n",
        "        out, _ = self.lstm(x)       # (B, 1, H)\n",
        "        h = out[:, -1, :]           # (B, H)\n",
        "        return self.head(h).squeeze(1)  # (B,)\n",
        "\n",
        "# 6) Train y evaluación\n",
        "def train_epoch(model, loader, opt, lossf, device):\n",
        "    model.train()\n",
        "    tot = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = lossf(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        tot += loss.item() * xb.size(0)\n",
        "    return tot / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        ps.append(pred); ys.append(yb.numpy())\n",
        "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
        "    mse  = mean_squared_error(y, p)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae  = mean_absolute_error(y, p)\n",
        "    mp   = mape(y, p)\n",
        "    r2   = r2_score(y, p)\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mp, \"R2\": r2}\n",
        "\n",
        "# 7) Una corrida rápida (CPU): 3 épocas, hidden=64\n",
        "def run_one_lstm_cpu(seed: int, epochs=3, hidden=64):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = LSTMTabular(input_size=Xtr_d.shape[1], hidden=hidden, layers=1).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss  = torch.nn.MSELoss()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        _ = train_epoch(model, train_loader, opt, loss, device)\n",
        "\n",
        "    val = eval_metrics(model, valid_loader, device)\n",
        "    tst = eval_metrics(model, test_loader,  device)\n",
        "    return val, tst\n",
        "\n",
        "# 8) Ejecutar 15 corridas en paralelo (CPU)\n",
        "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
        "    delayed(run_one_lstm_cpu)(s, epochs=3, hidden=64) for s in range(15)\n",
        ")\n",
        "runs_val  = [v for v, t in results]\n",
        "runs_test = [t for v, t in results]\n",
        "\n",
        "# 9) Media ± Desv\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"LSTM_fast — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"LSTM_fast — Test :\", fmt_pm(mean_test, std_test))\n",
        "\n",
        "# 10) Guardar métricas agregadas (si tienes save_metrics del BLOQUE 0)\n",
        "save_metrics(\"LSTM_fast_15runs_pm\", mean_val, mean_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5OppYMy2ZCa",
        "outputId": "7999b9fa-e922-428f-8c17-865bcdef1ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  6.6min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM_fast — Valid: {'MSE': '409117060.27 ± 2659818.39', 'RMSE': '20226.54 ± 65.74', 'MAE': '12907.99 ± 75.69', 'MAPE': '850084864.00 ± 171774976.00', 'R2': '-0.670 ± 0.011'}\n",
            "LSTM_fast — Test : {'MSE': '385796844.80 ± 2553018.34', 'RMSE': '19641.61 ± 64.98', 'MAE': '12396.30 ± 74.90', 'MAPE': '874170368.00 ± 176641920.00', 'R2': '-0.644 ± 0.011'}\n",
            "✓ Métricas guardadas en /content/drive/MyDrive/Modelo/outputs/resultados_modelos.csv y en metrics_LSTM_fast_15runs_pm.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOQUE 7 TabTransformer"
      ],
      "metadata": {
        "id": "F_4mFz7obg83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# BLOQUE 7 — TabTransformer (15 corridas, CPU, rápido)\n",
        "# =========================\n",
        "\n",
        "# 0) Forzar CPU y desactivar Dynamo (evita errores en Colab con torch 2.x)\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
        "\n",
        "# 1) Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 2) Fallbacks por si faltan helpers del BLOQUE 0\n",
        "if 'mape' not in globals():\n",
        "    def mape(y_true, y_pred, eps=1e-8):\n",
        "        y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "        return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "if 'summarize_runs' not in globals():\n",
        "    def summarize_runs(dicts_list):\n",
        "        keys = dicts_list[0].keys()\n",
        "        mean = {k: float(np.mean([d[k] for d in dicts_list])) for k in keys}\n",
        "        std  = {k: float(np.std([d[k] for d in dicts_list], ddof=1)) for k in keys}\n",
        "        return mean, std\n",
        "\n",
        "if 'fmt_pm' not in globals():\n",
        "    def fmt_pm(mean, std, r2_dec=3):\n",
        "        out = {}\n",
        "        for k in mean.keys():\n",
        "            out[k] = f\"{mean[k]:.{r2_dec}f} ± {std[k]:.{r2_dec}f}\" if k==\"R2\" else f\"{mean[k]:.2f} ± {std[k]:.2f}\"\n",
        "        return out\n",
        "\n",
        "if 'save_metrics' not in globals():\n",
        "    def save_metrics(name, val_dict, test_dict):\n",
        "        print(f\"(Nota) save_metrics no definido; omitiendo guardado para {name}.\")\n",
        "\n",
        "# 3) Dataset tabular (categorías + numéricas) desde tus DataFrames ya preprocesados\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X_df, y, cat_cols, num_cols):\n",
        "        self.Xc = X_df[cat_cols].astype(np.int64).to_numpy()\n",
        "        self.Xn = X_df[num_cols].astype(np.float32).to_numpy() if len(num_cols) else np.empty((len(X_df),0), np.float32)\n",
        "        self.y  = np.asarray(y, dtype=np.float32)\n",
        "    def __len__(self):  return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        return torch.from_numpy(self.Xc[i]), torch.from_numpy(self.Xn[i]), torch.tensor(self.y[i])\n",
        "\n",
        "# 4) DataLoaders rápidos (CPU)\n",
        "def make_loaders(Xtr_df, ytr, Xva_df, yva, Xte_df, yte, bs=1024):\n",
        "    train_ds = TabDataset(Xtr_df, ytr, cat_features, num_features)\n",
        "    valid_ds = TabDataset(Xva_df, yva, cat_features, num_features)\n",
        "    test_ds  = TabDataset(Xte_df, yte, cat_features, num_features)\n",
        "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,  num_workers=0, pin_memory=False)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=2*bs, shuffle=False, num_workers=0, pin_memory=False)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=2*bs, shuffle=False, num_workers=0, pin_memory=False)\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "train_loader, valid_loader, test_loader = make_loaders(\n",
        "    X_train, y_train, X_valid, y_valid, X_test, y_test, bs=1024\n",
        ")\n",
        "\n",
        "# 5) Bloques básicos del TabTransformer (compacto para ir rápido)\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim); self.fn = fn\n",
        "    def forward(self, x): return self.fn(self.norm(x))\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=16, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner = heads * dim_head\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, inner*3, bias=False)\n",
        "        self.to_out = nn.Linear(inner, dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):                  # x: (B, N_cat, D)\n",
        "        h = self.heads\n",
        "        q,k,v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        def reshape(t): return t.view(t.size(0), t.size(1), h, -1).transpose(1,2)  # (B, H, N, d)\n",
        "        q,k,v = map(reshape, (q,k,v))\n",
        "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.drop(attn)\n",
        "        out = attn @ v                      # (B, H, N, d)\n",
        "        out = out.transpose(1,2).contiguous().view(x.size(0), x.size(1), -1)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim*mult),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim*mult, dim)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth=2, heads=4, dim_head=16, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mult=2, dropout=dropout))\n",
        "            ]) for _ in range(depth)\n",
        "        ])\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = x + attn(x)\n",
        "            x = x + ff(x)\n",
        "        return x\n",
        "\n",
        "# 6) Modelo TabTransformer compacto:\n",
        "#    - embeddings por campo categórico\n",
        "#    - transformer sobre tokens categóricos\n",
        "#    - concat con numéricas normalizadas (LayerNorm)\n",
        "#    - MLP a salida escalar\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(self, cardinalities, num_cont, dim=32, depth=2, heads=4, dim_head=16, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.num_cat = len(cardinalities)\n",
        "        self.num_cont = num_cont\n",
        "        # embeddings por campo\n",
        "        self.embeds = nn.ModuleList([\n",
        "            nn.Embedding(card, dim) for card in cardinalities\n",
        "        ])\n",
        "        self.transformer = Transformer(dim=dim, depth=depth, heads=heads, dim_head=dim_head, dropout=dropout)\n",
        "        self.norm_cont = nn.LayerNorm(num_cont) if num_cont>0 else nn.Identity()\n",
        "        mlp_in = dim*self.num_cat + (num_cont if num_cont>0 else 0)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(mlp_in, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x_categ, x_cont):\n",
        "        # x_categ: (B, N_cat) con ids; x_cont: (B, N_num)\n",
        "        toks = [emb(x_categ[:,i]) for i,emb in enumerate(self.embeds)]  # lista de (B, D)\n",
        "        x_tok = torch.stack(toks, dim=1)  # (B, N_cat, D)\n",
        "        x_tok = self.transformer(x_tok)   # (B, N_cat, D)\n",
        "        x_tok = x_tok.reshape(x_tok.size(0), -1)  # (B, N_cat*D)\n",
        "        if self.num_cont>0:\n",
        "            xc = self.norm_cont(x_cont)\n",
        "            x = torch.cat([x_tok, xc], dim=-1)\n",
        "        else:\n",
        "            x = x_tok\n",
        "        return self.head(x).squeeze(1)\n",
        "\n",
        "# 7) Entrenamiento/evaluación (rápidos en CPU)\n",
        "def train_epoch(model, loader, opt, lossf, device):\n",
        "    model.train(); tot=0.0\n",
        "    for xc, xn, y in loader:\n",
        "        xc, xn, y = xc.to(device), xn.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(xc, xn)\n",
        "        loss = lossf(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        tot += loss.item()*y.size(0)\n",
        "    return tot/len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval(); ys=[]; ps=[]\n",
        "    for xc, xn, y in loader:\n",
        "        xc, xn = xc.to(device), xn.to(device)\n",
        "        p = model(xc, xn).cpu().numpy()\n",
        "        ps.append(p); ys.append(y.numpy())\n",
        "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
        "    mse  = mean_squared_error(y, p)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae  = mean_absolute_error(y, p)\n",
        "    mp   = mape(y, p)\n",
        "    r2   = r2_score(y, p)\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mp, \"R2\": r2}\n",
        "\n",
        "# 8) Una corrida rápida (3 épocas, dim=32, depth=2)\n",
        "def run_one_tabtx(seed:int, epochs=3, dim=32, depth=2, heads=4):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    device = torch.device(\"cpu\")\n",
        "    # cardinalidades en el mismo orden que cat_features\n",
        "    # (tu pre_process ya dejó enteros 0..(card-1))\n",
        "    # usa num_of_unique que viene en el mismo orden ['Store','Dept','Type'] ∩ cat_features\n",
        "    # alineamos por nombre:\n",
        "    name_to_card = dict(zip(['Store','Dept','Type'], num_of_unique))\n",
        "    cards = [int(name_to_card[c]) for c in cat_features]\n",
        "    model = TabTransformer(cardinalities=cards, num_cont=len(num_features),\n",
        "                           dim=dim, depth=depth, heads=heads).to(device)\n",
        "    opt  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss = nn.MSELoss()\n",
        "    for _ in range(epochs):\n",
        "        _ = train_epoch(model, train_loader, opt, loss, device)\n",
        "    val = evaluate(model, valid_loader, device)\n",
        "    tst = evaluate(model, test_loader,  device)\n",
        "    return val, tst\n",
        "\n",
        "# 9) Ejecutar 15 corridas en paralelo (CPU)\n",
        "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
        "    delayed(run_one_tabtx)(s, epochs=3, dim=32, depth=2, heads=4) for s in range(15)\n",
        ")\n",
        "runs_val  = [v for v,t in results]\n",
        "runs_test = [t for v,t in results]\n",
        "\n",
        "# 10) Media ± Desv\n",
        "mean_val, std_val   = summarize_runs(runs_val)\n",
        "mean_test, std_test = summarize_runs(runs_test)\n",
        "\n",
        "print(\"TabTransformer_fast — Valid:\", fmt_pm(mean_val, std_val))\n",
        "print(\"TabTransformer_fast — Test :\", fmt_pm(mean_test, std_test))\n",
        "\n",
        "# 11) Guardar métricas agregadas\n",
        "save_metrics(\"TabTransformer_fast_15runs_pm\", mean_val, mean_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnxCk3eq9aQi",
        "outputId": "a63ad055-d8ed-46f2-f587-20bed618d386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 11.1min\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 16.9min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabTransformer_fast — Valid: {'MSE': '47061047.87 ± 8648948.59', 'RMSE': '6833.17 ± 628.60', 'MAE': '4311.93 ± 415.70', 'MAPE': '6249863168.00 ± 5541392896.00', 'R2': '0.808 ± 0.035'}\n",
            "TabTransformer_fast — Test : {'MSE': '41051326.67 ± 7601130.12', 'RMSE': '6381.41 ± 593.66', 'MAE': '4060.59 ± 388.34', 'MAPE': '3969446656.00 ± 2926473984.00', 'R2': '0.825 ± 0.032'}\n",
            "✓ Métricas guardadas en /content/drive/MyDrive/Modelo/outputs/resultados_modelos.csv y en metrics_TabTransformer_fast_15runs_pm.json\n"
          ]
        }
      ]
    }
  ]
}